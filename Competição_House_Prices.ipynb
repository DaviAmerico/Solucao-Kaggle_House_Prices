{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Competição House Prices.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNhxy/dWnvG5ZFhGcIjx1jG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaviAmerico/project1/blob/main/Competi%C3%A7%C3%A3o_House_Prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOwZqfogMCc6"
      },
      "source": [
        "# Resolução da competição House Prices - Advanced Regression Techniques\r\n",
        "Nosso objetivo aqui é criar um modelo que preveja os valores de determinadas casas com base em 79 variáveis explorátorias, deveremos fazer isso usando um modelo de regressão.A resolução dessa competição se dará em três tópicos: **Limpeza dos dados**,**Transformando os dados** e **Criando o modelo e prevendo os preços**.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y15uZZ-FQlsc"
      },
      "source": [
        "**Limpeza dos dados**:\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tqdTjkcciwt"
      },
      "source": [
        "Nosso objetivo nessa seção é tratar os dados faltantes(*nan's*), mas antes vamos chamar os dados a serem usados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0PvyEhiR6xs"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from scipy import stats\r\n",
        "import xgboost as xgb\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "train=pd.read_csv('C:/Users/IWFY/Desktop/competição kaggle/train.csv')\r\n",
        "test=pd.read_csv('C:/Users/IWFY/Desktop/competição kaggle/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7y1rCd7SBiN"
      },
      "source": [
        "Nas sete primeiras linhas estamos importando os pacotes a serem usados , nas duas últimas estamos guardando em *train* os dados que serão usados para criar o modelo , em *test* estamos guardando os dados que serão usados para prever os preços."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0UPjZ0jSvwF"
      },
      "source": [
        "Agora vamos procurar dados faltantes nos dois conjuntos de dados:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZbh6jVETHPO",
        "outputId": "8238e411-1c22-4b44-9df0-924219a7a4fd"
      },
      "source": [
        "train.isna().sum().sum()\r\n",
        "test.isna().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT9vjfKgUnPY"
      },
      "source": [
        "No conjunto *train* temos 6965 valores faltantes e no conjunto *test* temos 7000.Inicialmente vamos tratar os *nan's* nas colunas númericas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf7Wqz_pVVbp"
      },
      "source": [
        "train.fillna(train.mean(),inplace=True)\r\n",
        "test.fillna(test.mean(),inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HyJpx-HVnfO"
      },
      "source": [
        "Acima substituimos os valores faltantes das colunas númericas pela média dos valores em cada coluna.Pelo fato de termos usado a média as colunas categóricas não foram alteradas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7g_qfkrW9xo"
      },
      "source": [
        "Agora vamos tratar os dados faltantes categóricos, infelizmente não é tão simples quanto para dados númericos , teremos que usar uma recursividade para substituí-los:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfU7qqELW_Oz"
      },
      "source": [
        "for i in train.select_dtypes('object').columns:\r\n",
        "    train.loc[:,i].fillna(train.loc[:,i].value_counts().index[0],inplace=True)\r\n",
        "    test.loc[:,i].fillna(test.loc[:,i].value_counts().index[0],inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usWZG3_VXDXp"
      },
      "source": [
        "Acima estamos percorrendo *i* nas colunas categóricas dos dados , para cada coluna os *nan's* serão substituidos pela categoria mais frequente da respectiva coluna."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA6c55M3XqnD"
      },
      "source": [
        "Podemos ver então que não existem mais valores faltante nos dois conjuntos de dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWJzkQngXxV-"
      },
      "source": [
        "train.isna().sum().sum()\r\n",
        "test.isna().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfOg8uRgYKUV"
      },
      "source": [
        "**Transformando os dados:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y8ACLHYYVGE"
      },
      "source": [
        "A maioria dos algoritmos de aprendizado de maquina só aceitam dados númericos e funcionam melhor com dados normalizados.Abaixo vamos transformar todas categorias em números:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmg0txHJY81u"
      },
      "source": [
        "for i in train.select_dtypes('object').columns:\r\n",
        "    train.loc[:,i]=LabelEncoder().fit_transform(train.loc[:,i].astype('str'))\r\n",
        "    test.loc[:,i]=LabelEncoder().fit_transform(test.loc[:,i].astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uKnZe9FZDjy"
      },
      "source": [
        "Podemos ver agora que cada coluna dos dois conjuntos é númerica:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zezmT_FZKkq"
      },
      "source": [
        "train.dtypes\r\n",
        "test.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XsLj6UdZXRY"
      },
      "source": [
        "Antes de normalizar os dados vamos separar a variável resposta das explicativas , pois estamos interessados em apenas normalizar as explicativas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf60bt6KaI4S"
      },
      "source": [
        "xt=train.iloc[:,0:80]\r\n",
        "yt=train.iloc[:,80]\r\n",
        "xte=test\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAwWRX9RaUNd"
      },
      "source": [
        "Em *xt* e *xte* temos as variáveis explicativas a serem normalizadas nos conjuntos *train* e *test*, note que *xte* é igual a *test*,pois o conjunto de dados *test* não contém observações da variável resposta , até porque queremos prevê-las para essa conjunto. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6aq7DL1jULC"
      },
      "source": [
        "Normalizando os dados, exceto a resposta, temos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kT2jgakbYlb"
      },
      "source": [
        "zt=pd.DataFrame(StandardScaler().fit_transform(xt))\r\n",
        "zt.columns=xt.columns\r\n",
        "zte=pd.DataFrame(StandardScaler().fit_transform(xte))\r\n",
        "zte.columns=xte.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDrPejP9bs3E"
      },
      "source": [
        "*zt* e *zte* são os preditores normalizados para ambos os conjuntos de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfd8WBu8b-_z"
      },
      "source": [
        "Agora estamos preparados para criar nosso modelo e fazer as predições :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEHnqC-zcDmA"
      },
      "source": [
        "**Criando o modelo e prevendo os preços:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykRSNobLcmO4"
      },
      "source": [
        "O modelo a ser usado para resolver o problema de regressão é o *XGBoosting(Extreme Gradient Boosting)*, e ele será treinado em *(zt,yt)*.Iremos testar várias combinações aleatórias de hiper-parâmetros nele e pegar a combinação ótima depois de um determinado número de tentativas,no caso são 25 tentativas, para nós a combinação ótima é aquela que obteve a menor raiz do erro quadrático logarítmico médio,métrica de avaliação usada na competição, dentro da validação cruzada com 5 *folds*,então:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ewJngocd0g6"
      },
      "source": [
        "xgbr=xgb.XGBRegressor()\r\n",
        "param_grid = {'n_estimators': stats.randint(150, 500),\r\n",
        "              'learning_rate': stats.uniform(0.01, 0.07),\r\n",
        "              'subsample': stats.uniform(0.3, 0.7),\r\n",
        "              'max_depth': [3, 4, 5, 6, 7, 8, 9],\r\n",
        "              'colsample_bytree': stats.uniform(0.45,0.5),\r\n",
        "              'min_child_weight': [1, 2, 3]\r\n",
        "             }\r\n",
        "f=RandomizedSearchCV(xgbr,param_grid,cv=5,n_iter=25,scoring='neg_mean_squared_log_error')\r\n",
        "f.fit(zt,yt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCbZcWs_eK6c"
      },
      "source": [
        "Podemos,também, verificar o erro da melhor combinação de hiper-parâmetros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwAuyllJfYFD"
      },
      "source": [
        "bs=np.sqrt(-f.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaUmfhGtfhX7"
      },
      "source": [
        "Agora podemos fazer a previsão dos preços das casas dentro do conjunto *test* normalizado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeoHjkYRf07b"
      },
      "source": [
        "yp=f.best_estimator_.predict(zte)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZ24bDhl6el"
      },
      "source": [
        "Agora basta exportar as predições para o Kaggle para concluir a competição :D"
      ]
    }
  ]
}